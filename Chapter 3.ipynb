{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Processing Pipelines\n",
    "\n",
    "This chapter is dedicated to processing pipelines: a series of functions applied to a Doc to add attributes like part-of-speech tags, dependency labels or named entities.In this lesson, you'll learn about the pipeline components provided by spaCy, and what happens behind the scenes when you call nlp on a string of text.\n",
    "\n",
    "You've already written this plenty of times by now: pass a string of text to the nlp object, and receive a Doc object. But what does the nlp object actually do? First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the Doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed Doc is returned, so you can work with it.\n",
    "![pipeline](fig/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy ships with the following built-in pipeline components.\n",
    "\n",
    "- **The part-of-speech tagger** sets the `token.tag` attribute.\n",
    "- **The dependency parser** adds the `token.dep` and `token.head` attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "- **The named entity recognizer** adds the detected entities to the `doc.ents` property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "- The **text classifier** sets category labels that apply to the whole text, and adds them to the `doc.cats` property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system.\n",
    "![pipeline components](fig/pipeline_components.png)\n",
    "\n",
    "All models you can load into spaCy include several files and a meta JSON. The meta defines things like the language and pipeline. This tells spaCy which components to instantiate. The built-in components that make predictions also need binary data. The data is included in the model package and loaded into the component when you load the model.\n",
    "![under the hood](fig/under_the_hood.png)\n",
    "\n",
    "To see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute. For a list of component name and component function tuples, you can use the `nlp.pipeline` attribute. The component functions are the functions applied to the Doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7ff5a9dbccf8>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7ff5a80f6228>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7ff5a80f6288>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "Now that you know how spaCy's pipeline works, let's take a look at another very powerful feature: custom pipeline components. Custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the nlp object on a text – for example, to modify the Doc and add more data to it.\n",
    "\n",
    "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own. Custom components are executed automatically when you call the nlp object on a text. They're especially useful for adding your own custom metadata to documents and tokens. You can also use them to update built-in attributes, like the named entity spans. In other words they allow to:\n",
    "1. Make a function execute automatically when you call `nlp`.\n",
    "2. Add your own metadata to documents and tokens.\n",
    "3. Update built-in attributes like `doc.ents`.\n",
    "\n",
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline. Components can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the component function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def custom_component(doc):\n",
    "    # Do something to the document\n",
    "    return doc\n",
    "\n",
    "# Add the new custom component to the pipeline.\n",
    "nlp.add_pipe(custom_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify where to add the component in the pipeline, you can use the following keyword arguments: \n",
    "- Setting `last` to `True` will add the component last in the pipeline. This is the default behavior. \n",
    "- Setting `first` to `True` will add the component first in the pipeline, right after the tokenizer.\n",
    "- The `before` and `after` arguments let you define the name of an existing component to add the new component before or after. For example, before equals \"ner\" will add it before the named entity recognizer.\n",
    "\n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error.\n",
    "\n",
    "| Argument | Description | Example |\n",
    "|----------|-------------|---------|\n",
    "|`last`    |\tIf `True`, add last\t | `nlp.add_pipe(component, last=True)` |\n",
    "|`first`   |\tIf `True`, add first | `nlp.add_pipe(component, first=True)` |\n",
    "|`before`  |\tAdd before component | `nlp.add_pipe(component, before='ner')` |\n",
    "|`after`   |\tAdd after component  | `nlp.add_pipe(component, after='tagger')` |\n",
    "\n",
    "Here's an example of a simple pipeline component. We start off with the small English model. We then define the component – a function that takes a `Doc` object and returns it. Let's do something simple and print the length of the `Doc` that passes through the pipeline. Don't forget to return the `Doc` so it can be processed by the next component in the pipeline! The `Doc` created by the tokenizer is passed through all components, so it's important that they all return the modified `Doc`. We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting `first` equals `True`. When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    \"\"\"Print the doc's length.\"\"\"\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we process a text using the nlp object, the custom component will be applied to the Doc and the length of the document will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "Define the custom component and apply the matcher to the doc.\n",
    "Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans.\n",
    "Add the new component to the pipeline after the 'ner' component.\n",
    "Process the text and print the entity text and entity label for the entities in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 6303828839600189595), ('Golden Retriever', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('ANIMAL', None, *animal_patterns)\n",
    "\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Attributes\n",
    "\n",
    "In this lesson, we'll learn how to add custom attributes to the Doc, Token and Span objects to store custom data. Custom attributes let you add any meta data to Docs, Tokens and Spans. The data can be added once, or it can be computed dynamically. Custom attributes are available via the `._.` property. This makes it clear that they were added by the user, and not built into spaCy, like `token.text`. Attributes need to be registered on the global Doc, Token and Span classes you can import from `spacy.tokens`. You've already worked with those in the previous chapters. To register a custom attribute on the Doc, Token or Span, you can use the `set_extension` method. The first argument is the attribute name. Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten.\n",
    "\n",
    "First we need to register the extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and only after registering, they can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My document True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('This is a sentence')\n",
    "token = doc[0]\n",
    "span = doc[0:2]\n",
    "\n",
    "doc._.title = 'My document'\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "\n",
    "print(doc._.title, token._.is_color, span._.has_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of extensions: \n",
    "- attribute extensions\n",
    "- property extensions\n",
    "- method extensions\n",
    "\n",
    "### Attribute extensions\n",
    "Attribute extensions set a default value that can be overwritten. For example, a custom `is_color` attribute on the token that defaults to `False`. On individual tokens, its value can be changed by overwriting it – in this case, `True` for the token \"blue\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extension on the Token with default value\n",
    "# Note that we must add the `force=True` option because this extension has been set\n",
    "# in the previous cell\n",
    "Token.set_extension('is_color', default=False, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "\n",
    "Property extensions work like properties in Python: they can define a getter function and an optional setter. The getter function is only called when you retrieve the attribute. This lets you compute the value dynamically, and even take other custom attributes into account. Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors. We can then provide the function via the getter keyword argument when we register the extension. The token \"blue\" now returns `True` for \"is color\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to set extension attributes on a `Span`, you almost always want to use a property extension with a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values. In this example, the `get_has_color` function takes the span and returns whether the text of any of the tokens is in the list of colors. After we've processed the doc, we can check different slices of the doc and the custom `has_color` property returns whether the span contains a color token or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color', getter=get_has_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method extensions\n",
    "\n",
    "Method extensions make the extension attribute a callable method. You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting. In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the Doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, `token_text`. Here, the custom `has_token` method returns `True` for the word \"blue\" and `False` for the word \"cloud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
