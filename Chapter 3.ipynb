{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "\n",
    "## Processing Pipelines\n",
    "\n",
    "This chapter is dedicated to processing pipelines: a series of functions applied to a Doc to add attributes like part-of-speech tags, dependency labels or named entities. In this lesson, you'll learn about the pipeline components provided by spaCy, and what happens behind the scenes when you call nlp on a string of text.\n",
    "\n",
    "### What happens when you call `nlp`?\n",
    "\n",
    "You've already written this plenty of times by now: pass a string of text to the nlp object, and receive a Doc object. But what does the nlp object actually do? First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the Doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed Doc is returned, so you can work with it.\n",
    "![pipeline](fig/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in pipeline components\n",
    "\n",
    "spaCy ships with the following built-in pipeline components.\n",
    "\n",
    "- **The part-of-speech tagger** sets the `token.tag` attribute.\n",
    "- **The dependency parser** adds the `token.dep` and `token.head` attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "- **The named entity recognizer** adds the detected entities to the `doc.ents` property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "- The **text classifier** sets category labels that apply to the whole text, and adds them to the `doc.cats` property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system.\n",
    "![pipeline components](fig/pipeline_components.png)\n",
    "\n",
    "### Under the hood\n",
    "\n",
    "All models you can load into spaCy include several files and a meta JSON. The meta defines things like the language and pipeline. This tells spaCy which components to instantiate. The built-in components that make predictions also need binary data. The data is included in the model package and loaded into the component when you load the model.\n",
    "![under the hood](fig/under_the_hood.png)\n",
    "\n",
    "### Pipeline attributes \n",
    "\n",
    "To see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute. For a list of component name and component function tuples, you can use the `nlp.pipeline` attribute. The component functions are the functions applied to the Doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x1129406a0>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x1161e1468>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x1161e14c8>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "Now that you know how spaCy's pipeline works, let's take a look at another very powerful feature: custom pipeline components. Custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the nlp object on a text – for example, to modify the Doc and add more data to it.\n",
    "\n",
    "### Why custom components?\n",
    "\n",
    "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own. Custom components are executed automatically when you call the nlp object on a text. They're especially useful for adding your own custom metadata to documents and tokens. You can also use them to update built-in attributes, like the named entity spans. In other words they allow to:\n",
    "1. Make a function execute automatically when you call `nlp`.\n",
    "2. Add your own metadata to documents and tokens.\n",
    "3. Update built-in attributes like `doc.ents`.\n",
    "\n",
    "### Anatomy of a component\n",
    "\n",
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline. Components can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the component function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def custom_component(doc):\n",
    "    # Do something to the document\n",
    "    return doc\n",
    "\n",
    "# Add the new custom component to the pipeline.\n",
    "nlp.add_pipe(custom_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify where to add the component in the pipeline, you can use the following keyword arguments: \n",
    "- Setting `last` to `True` will add the component last in the pipeline. This is the default behavior. \n",
    "- Setting `first` to `True` will add the component first in the pipeline, right after the tokenizer.\n",
    "- The `before` and `after` arguments let you define the name of an existing component to add the new component before or after. For example, `before='ner'` will add it before the named entity recognizer.\n",
    "\n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error.\n",
    "\n",
    "| Argument | Description | Example |\n",
    "|----------|-------------|---------|\n",
    "|`last`    |\tIf `True`, add last\t | `nlp.add_pipe(component, last=True)` |\n",
    "|`first`   |\tIf `True`, add first | `nlp.add_pipe(component, first=True)` |\n",
    "|`before`  |\tAdd before component | `nlp.add_pipe(component, before='ner')` |\n",
    "|`after`   |\tAdd after component  | `nlp.add_pipe(component, after='tagger')` |\n",
    "\n",
    "\n",
    "### Example: a simple component\n",
    "\n",
    "Here's an example of a simple pipeline component. We start off with the small English model. We then define the component – a function that takes a `Doc` object and returns it. Let's do something simple and print the length of the `Doc` that passes through the pipeline. Don't forget to return the `Doc` so it can be processed by the next component in the pipeline! The `Doc` created by the tokenizer is passed through all components, so it's important that they all return the modified `Doc`. We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting `first` equals `True`. When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    \"\"\"Print the doc's length.\"\"\"\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we process a text using the nlp object, the custom component will be applied to the Doc and the length of the document will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "Define the custom component and apply the matcher to the doc.\n",
    "Create a Span for each match, assign the label ID for 'ANIMAL' and overwrite the doc.ents with the new spans.\n",
    "Add the new component to the pipeline after the 'ner' component.\n",
    "Process the text and print the entity text and entity label for the entities in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 6303828839600189595), ('Golden Retriever', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('ANIMAL', None, *animal_patterns)\n",
    "\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Attributes\n",
    "\n",
    "In this lesson, we'll learn how to add custom attributes to the Doc, Token and Span objects to store custom data.\n",
    "\n",
    "### Setting custom attributes\n",
    "\n",
    "- Custom attributes let you add any meta data to Docs, Tokens and Spans. \n",
    "- The data can be added once, or it can be computed dynamically. \n",
    "- Custom attributes are available via the `._.` property. This makes it clear that they were added by the user, and not built into spaCy, like `token.text`. \n",
    "- Attributes need to be registered on the global Doc, Token and Span classes you can import from `spacy.tokens`. \n",
    "\n",
    "To register a custom attribute on the Doc, Token or Span, you can use the `set_extension` method. \n",
    "\n",
    "- The first argument is the attribute name. \n",
    "- Keyword arguments let you define how the value should be computed. In the examples below, it has a default value and can be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Register extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the extensions are registered, we can set them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My document True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('This is a sentence')\n",
    "token = doc[0]\n",
    "span = doc[0:2]\n",
    "\n",
    "doc._.title = 'My document'\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "\n",
    "print(doc._.title, token._.is_color, span._.has_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attribute types\n",
    "\n",
    "There are three types of extensions: \n",
    "- Attribute extensions\n",
    "- Property extensions\n",
    "- Method extensions\n",
    "\n",
    "### Attribute extensions\n",
    "Attribute extensions set a default value that can be overwritten. For example, a custom `is_color` attribute on the token that defaults to `False`. On individual tokens, its value can be changed by overwriting it – in this case, `True` for the token \"blue\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set extension on the Token with default value\n",
    "# Note that we must add the `force=True` option because this extension has been set\n",
    "# in the previous cell\n",
    "Token.set_extension('is_color', default=False, force=True)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "\n",
    "Property extensions work like properties in Python: they can define a getter function and an optional setter. The getter function is only called when you retrieve the attribute. This lets you compute the value dynamically, and even take other custom attributes into account. Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors. We can then provide the function via the getter keyword argument when we register the extension. The token \"blue\" now returns `True` for \"is color\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to set extension attributes on a `Span`, you almost always want to use a property extension with a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values. In this example, the `get_has_color` function takes the span and returns whether the text of any of the tokens is in the list of colors. After we've processed the doc, we can check different slices of the doc and the custom `has_color` property returns whether the span contains a color token or not.\n",
    "\n",
    "Note that, despite being methods, these extensions do not accept arguments other than token, span etc. If we want to be able to pass external arguments, we need to use *method extensions*, described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color', getter=get_has_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method extensions\n",
    "\n",
    "Method extensions make the extension attribute a callable method. You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting. In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the Doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, `token_text`. Here, the custom `has_token` method returns `True` for the word \"blue\" and `False` for the word \"cloud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a few examples.\n",
    "\n",
    "* Use Token.set_extension to register is_country (default False).\n",
    "* Update it for \"Spain\" and print it for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute 'is_country' with the default \n",
    "# value False\n",
    "Token.set_extension('is_country', default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for \n",
    "# the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second example:\n",
    "- Use `Token.set_extension` to register `reversed` (getter function `get_reversed`).\n",
    "- Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the has_number function .\n",
    "- Use `Doc.set_extension` to register `has_number` (getter `get_has_number`) and print its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of method extension.\n",
    "\n",
    "* Use `Span.set_extension` to register `to_html` (method `to_html`).\n",
    "* Call it on `doc[0:2]` with the tag `strong`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define a to_html method that takes a span and a tags argument.\n",
    "def to_html(span, tag):\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the method with the Span class\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "doc = nlp('Hello world, this is a sentence.')\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we create a `get_wikipedia_url` getter so it only returns the URL if the span’s label is in the list of labels.\n",
    "- Set the Span extension 'wikipedia_url' using the getter - `get_wikipedia_url`.\n",
    "- Iterate over the entities in the doc and output their Wikipedia URL.\n",
    "\n",
    "**TO DO** Understand `span.label_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    \"\"\"Get the Wikipedia URL of a span if it contains one of the following\n",
    "    labels: 'PERSON', 'ORG', 'GPE', 'LOCATION'\n",
    "    \"\"\"\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return 'https://en.wikipedia.org/w/index.php?search=' + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension attributes are especially powerful if they’re combined with custom pipeline components. In this exercise, you’ll write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available.\n",
    "A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable `CAPITALS`.\n",
    "In the example below, we  Span with the label `'GPE'` (geopolitical entity) for all matches. The steps are:\n",
    "\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute 'capital' with the getter get_capital.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/capitals.json\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital\n",
    "Span.set_extension('capital', getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Performance\n",
    "\n",
    "In this lesson, I'll show you a few tips and tricks to make your spaCy pipelines run as fast as possible, and process large volumes of text efficiently.\n",
    "\n",
    "### Processing large volumes of text\n",
    "\n",
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the `nlp.pipe` method can speed this up significantly. It processes the texts as a stream and yields `Doc` objects. It is much faster than just calling `nlp` on each text, because it batches up the texts. `nlp.pipe` is a generator that yields `Doc` objects, so in order to get a list of `Doc`s, remember to call the `list` method around it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# WRONG way of processing lots of text\n",
    "doc = [nlp(text) for text in LOTS_OF_TEXT]\n",
    "\n",
    "# RIGHT way of processing lots of text\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing in context\n",
    "\n",
    "`nlp.pipe` also supports passing in tuples of text / context if you set `as_tuples` to True.\n",
    "The method will then yield doc / context tuples. This is useful for passing in additional metadata, like an ID associated with the text, or a page number. In the example below we have two documents each with a dictionary of metadata consisting of an `id` and a `page_number` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "# Setting as_tuples=True on nlp.pipe lets you pass in (text, context) tuples\n",
    "# Yields (doc, context) tuples\n",
    "# Useful for associating metadata with the doc\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even add the context meta data to custom attributes. In this example, we're registering two extensions, `id` and `page_number`, which default to `None`.\n",
    "After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Register the two extension with the Doc class.\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "# Set the attributes with the context metadata.\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only the tokenizer\n",
    "\n",
    "Another common scenario: Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text. Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need. If you only need a tokenized `Doc` object, you can use the `nlp.make_doc` method instead, which takes a text and returns a `Doc`.\n",
    "This is also how spaCy does it behind the scenes: `nlp.make_doc` turns the text into a `Doc` before the pipeline components are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD if you only want tokenized text\n",
    "doc = nlp('Hello world')\n",
    "\n",
    "# GOOD if you only want tokenized text\n",
    "doc = nlp.make_doc('Hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling pipeline components\n",
    "\n",
    "spaCy also allows you to temporarily disable pipeline components using the `nlp.disable_pipes` context manager. It takes a variable number of arguments, the string names of the pipeline components to disable. For example, if you only want to use the entity recognizer to process a document, you can temporarily disable the tagger and parser. After the `with` block, the disabled pipeline components are automatically restored. In the `with` block, spaCy will only run the remaining components."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing streams\n",
    "\n",
    "In the example below we read a collection of tweets using `nlp.pipe` and iterate on each `doc` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[McDonalds is my favorite restaurant.,\n",
       " Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..,\n",
       " People really still eat McDonalds :(,\n",
       " The McDonalds in Spain has chicken wings. My heart is so happy ,\n",
       " @McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P,\n",
       " please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D,\n",
       " This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/tweets.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing with context\n",
    "\n",
    "In this exercise, you’ll be using custom attributes to add author and book meta information to quotes. A list of [text, context] examples is available as the variable `DATA`. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'.\n",
    "Use the `set_extension` method to register the custom attributes `author` and `book` on the `Doc`, which default to `None`.\n",
    "Process the [text, context] pairs in `DATA` using `nlp.pipe` with `as_tuples=True`.\n",
    "Overwrite the `doc._.book` and `doc._.author` with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/bookquotes.json\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author', default=None)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective processing\n",
    "\n",
    "In this exercise, you’ll use the `nlp.make_doc` and `nlp.disable_pipes` methods to only run selected components when processing a text.\n",
    "\n",
    "Part 1\n",
    "\n",
    "Rewrite the code to only tokenize the text using `nlp.make_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "Disable the tagger and parser using the nlp.disable_pipes method.\n",
    "Process the text and print all entities in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
