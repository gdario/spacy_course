{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a neural network model\n",
    "\n",
    "## Training and updating models\n",
    "\n",
    "Welcome to the final chapter, which is about one of the most exciting aspects of modern NLP: training your own models!\n",
    "In this lesson, you'll learn about training and updating spaCy's neural network models and the data you need for it – focusing specifically on the named entity recognizer.\n",
    "\n",
    "### Why updating the model?\n",
    "\n",
    "Before we get starting with explaining how, it's worth taking a second to ask ourselves: Why would we want to update the model with our own examples? Why can't we just rely on pre-trained models? Statistical models make predictions based on the examples they were trained on. You can usually make the model more accurate by showing it examples from your domain. You often also want to predict categories specific to your problem, so the model needs to learn about them. This is essential for text classification, very useful for entity recognition and a little less critical for tagging and parsing. In brief, the main benefits of updating a model are:\n",
    "\n",
    "- Better results on your specific domain\n",
    "- Learn classification schemes specifically for your problem\n",
    "- Essential for text classification\n",
    "- Very useful for named entity recognition\n",
    "- Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "### How training works\n",
    "\n",
    "spaCy supports updating existing models with more examples, and training new models. If we're not starting with a pre-trained model, we first initialize the weights randomly. Next, we call `nlp.update`, which predicts a batch of examples with the current weights. The model then checks the predictions against the correct answers, and decides how to change the weights to achieve better predictions next time. Finally, we make a small correction to the current weights and move on to the next batch of examples. We continue calling nlp dot update for each batch of examples in the data. In brief, the loop is:\n",
    "\n",
    "1. Initialize the model weights randomly with nlp.begin_training\n",
    "2. Predict a few examples with the current weights by calling nlp.update\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weights slightly\n",
    "6. Go back to 2.\n",
    "\n",
    "Here's an illustration showing the process. The training data are the examples we want to update the model with. The **text** should be a sentence, paragraph or longer document. For the best results, it should be similar to what the model will see at runtime. The **label** is what we want the model to predict. This can be a text category, or an entity span and its type. The gradient is how we should change the model to reduce the current error. It's computed when we compare the predicted label to the true label. After training, we can then save out an updated model and use it in our application.\n",
    "![how_training_works](fig/training.png)\n",
    "\n",
    "### Example: Training the entity recognizer\n",
    "\n",
    "Let's look at an example for a specific component: the entity recognizer. The entity recognizer takes a document and predicts phrases and their labels. This means that the training data needs to include texts, the entities they contain, and the entity labels. Entities can't overlap, so each token can only be part of one entity. Because the entity recognizer predicts entities in context, it also needs to be trained on entities and their surrounding context. The easiest way to do this is to show the model a text and a list of character offsets. For example, \"iPhone X\" is a gadget, starts at character 0 and ends at character 8. It's also very important for the model to learn words that aren't entities. In this case, the list of span annotations will be empty. Our goal is to teach the model to recognize new entities in similar contexts, even if they weren't in the training data.\n",
    "\n",
    "- The entity recognizer tags words and phrases in context\n",
    "- Each token can only be part of one entity\n",
    "- Examples need to come with context\n",
    "\n",
    "`(\"iPhone X is coming\", {'entities': [(0, 8, 'GADGET')]})`\n",
    "\n",
    "Texts with no entities are also important\n",
    "\n",
    "`(\"I need a new phone! Any tips?\", {'entities': []})`\n",
    "\n",
    "Goal: teach the model to generalize\n",
    "\n",
    "### The training data\n",
    "\n",
    "The training data tells the model what we want it to predict. This could be texts and named entities we want to recognize, or tokens and their correct part-of-speech tags. To update an existing model, we can start with a few hundred to a few thousand examples. To train a new category we may need up to a million. spaCy's pre-trained English models for instance were trained on 2 million words labelled with part-of-speech tags, dependencies and named entities. Training data is usually created by humans who assign labels to texts. This is a lot of work, but can be semi-automated – for example, using spaCy's `Matcher`.\n",
    "\n",
    "- Examples of what we want the model to predict in context\n",
    "- Update an existing model: a few hundred to a few thousand examples\n",
    "- Train a new category: a few thousand to a million examples\n",
    "    - spaCy's English models: 2 million words\n",
    "- Usually created manually by human annotators\n",
    "- Can be semi-automated – for example, using spaCy's `Matcher`!\n",
    "\n",
    "### Creating training data (1)\n",
    "\n",
    "spaCy’s rule-based `Matcher` is a great way to quickly create training data for named entity models. A list of sentences is available as the variable `TEXTS`. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as `'GADGET'`.\n",
    "\n",
    "- Write a pattern for two tokens whose lowercase forms match `'iphone'` and `'x'`.\n",
    "- Write a pattern for two tokens: one token whose lowercase form matches 'iphone' and an optional digit using the `'?'` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the match patterns we’ve created in the previous exercise to bootstrap a set of training examples. A list of sentences is available as the variable `TEXTS`.\n",
    "\n",
    "- Create a doc object for each text using nlp.pipe.\n",
    "- Match on the doc and create a list of matched spans.\n",
    "- Get (start character, end character, label) tuples of matched spans.\n",
    "- Format each example as a tuple of the text and a dict, mapping 'entities' to the entity tuples.\n",
    "- Append the example to `TRAINING_DATA` and inspect the printed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "While some other libraries give you one method that takes care of training a model, spaCy gives you full control over the training loop.\n",
    "\n",
    "### The steps of a training loop\n",
    "\n",
    "The training loop is a series of steps that's performed to train or update a model. We usually need to perform it several times, for multiple iterations, so that the model can learn from it effectively. If we want to train for 10 iterations, we need to loop 10 times. To prevent the model from getting stuck in a suboptimal solution, we randomly shuffle the data for each iteration. This is a very common strategy when doing stochastic gradient descent. Next, we divide the training data into batches of several examples, also known as minibatching. This makes it easier to make a more accurate estimate of the gradient. Finally, we update the model for each batch, and start the loop again until we've reached the last iteration. We can then save the model to a directory and use it in spaCy. In summary, the steps of a training loop are:\n",
    "\n",
    "1. **Loop** for a number of times.\n",
    "2. **Shuffle** the training data.\n",
    "3. **Divide** the data into batches.\n",
    "4. **Update** the model for each batch.\n",
    "5. **Save** the updated model.\n",
    "\n",
    "### Recap: how training works\n",
    "\n",
    "To recap: \n",
    "\n",
    "- The training data are the examples we want to update the model with. \n",
    "- The text should be a sentence, paragraph or longer document. For the best results, it should be similar to what the model will see at runtime.\n",
    "- The label is what we want the model to predict. This can be a text category, or an entity span and its type.\n",
    "- The gradient is how we should change the model to reduce the current error. It's computed when we compare the predicted label to the true label.\n",
    "\n",
    "Or, even more schematically:\n",
    "\n",
    "- **Training data**: Examples and their annotations.\n",
    "- **Text**: The input text the model should predict a label for.\n",
    "- **Label**: The label the model should predict.\n",
    "- **Gradient**: How to change the weights.\n",
    "\n",
    "### Example loop\n",
    "\n",
    "Here's an example. Let's imagine we have a list of training examples consisting of texts and entity annotations. We want to loop for 10 iterations, so we're iterating over a range of 10. Next, we use the random module to randomly shuffle the training data. We then use spaCy's minibatch utility function to divide the examples into batches. For each batch, we get the texts and annotations and call the nlp dot update method to update the model. Finally, we call the nlp dot to disk method to save the trained model to a directory."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    # And many more examples...\n",
    "]\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating an existing model\n",
    "\n",
    "spaCy lets you update an existing pre-trained model with more data – for example, to improve its predictions on different texts. This is especially useful if you want to improve categories the model already knows, like \"person\" or \"organization\". You can also update a model to add new categories. Just make sure to always update it with examples of the new category and examples of the other categories it previously predicted correctly. Otherwise improving the new category might hurt the other categories. In brief:\n",
    "\n",
    "- Improve the predictions on new data\n",
    "- Especially useful to improve existing categories, like PERSON\n",
    "- Also possible to add new categories\n",
    "- Be careful and make sure the model doesn't \"forget\" the old ones\n",
    "\n",
    "### Setting up a new pipeline from scratch\n",
    "\n",
    "In this example:\n",
    "\n",
    "1. we start off with a blank English model using the `spacy.blank` method. The blank model doesn't have any pipeline components, only the language data and tokenization rules. \n",
    "2. We then create a blank entity recognizer and add it to the pipeline.\n",
    "3. Using the `add_label` method, we can add new string labels to the model.\n",
    "4. We can now call `nlp_begin_training` to initialize the model with random weights.\n",
    "5. To get better accuracy, we want to loop over the examples more than once and randomly shuffle the data on each iteration.\n",
    "6. On each iteration, we divide the examples into batches using spaCy's minibatch utility function. Each example consists of a text and its annotations.\n",
    "7. Finally, we update the model with the texts and annotations and continue the loop."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "# Add a new label\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you’ll prepare a spaCy pipeline to train the entity recognizer to recognize 'GADGET' entities in a text – for example, “iPhone X”.\n",
    "\n",
    "- Create a blank 'en' model, for example using the spacy.blank method.\n",
    "- Create a new entity recognizer using nlp.create_pipe and add it to the pipeline.\n",
    "- Add the new label 'GADGET' to the entity recognizer using the add_label method on the pipeline component.\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write a simple training loop from scratch! The pipeline you’ve created in the previous exercise is available as the nlp object. It already contains the entity recognizer with the added label `GADGET`.\n",
    "The small set of labelled examples that you’ve created previously is available as `TRAINING_DATA`. To see the examples, you can print them in your script.\n",
    "Call `nlp.begin_training`, create a training loop for 10 iterations and shuffle the training data.\n",
    "Create batches of training data using `spacy.util.minibatch` and iterate over the batches.\n",
    "Convert the (text, annotations) tuples in the batch to lists of texts and annotations.\n",
    "For each batch, use `nlp.update` to update the model with the texts and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 8.79999989271164}\n",
      "{'ner': 20.951025545597076}\n",
      "{'ner': 31.465470612049103}\n",
      "{'ner': 8.098943054676056}\n",
      "{'ner': 13.147921711206436}\n",
      "{'ner': 17.55341798067093}\n",
      "{'ner': 1.6576312705874443}\n",
      "{'ner': 4.81340563390404}\n",
      "{'ner': 7.988197215367109}\n",
      "{'ner': 2.5478105792280985}\n",
      "{'ner': 3.3421422640094534}\n",
      "{'ner': 8.01981962856371}\n",
      "{'ner': 4.16334275493864}\n",
      "{'ner': 6.28494945296552}\n",
      "{'ner': 10.222646754584275}\n",
      "{'ner': 2.883513590320945}\n",
      "{'ner': 3.737090939161135}\n",
      "{'ner': 6.307621145126177}\n",
      "{'ner': 1.2385200823191553}\n",
      "{'ner': 2.6980711615178734}\n",
      "{'ner': 4.189065874760445}\n",
      "{'ner': 0.9585814378442592}\n",
      "{'ner': 1.2942532582204649}\n",
      "{'ner': 1.386440518661061}\n",
      "{'ner': 0.011443835996033158}\n",
      "{'ner': 2.0200232458666676}\n",
      "{'ner': 2.0287773504767492}\n",
      "{'ner': 1.20930335987927}\n",
      "{'ner': 1.2098468490623964}\n",
      "{'ner': 1.2107686258097377}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"exercises/gadgets.json\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices for training spaCy models\n",
    "\n",
    "When you start running your own experiments, you might find that a lot of things just don't work the way you want them to. And that's okay. Training models is an iterative process, and you have to try different things until you find out what works best. In this lesson, I'll be sharing some best practices and things to keep in mind when training your own models. Let's take a look at some of the problems you may come across.\n",
    "\n",
    "### Problem 1: models can forget things\n",
    "\n",
    "Statistical models can learn lots of things – but it doesn't mean that they won't unlearn them. If you're updating an existing model with new data, especially new labels, it can overfit and adjust too much to the new examples. For instance, if you're only updating it with examples of \"website\", it may \"forget\" other labels it previously predicted correctly – like \"person\". This is also known as the **catastrophic forgetting** problem.\n",
    "\n",
    "- Existing model can overfit on new data\n",
    "    - e.g.: if you only update it with WEBSITE, it can \"unlearn\" what a PERSON is\n",
    "- Also known as \"catastrophic forgetting\" problem\n",
    "\n",
    "### Solution 1: mix in previously correct solutions\n",
    "\n",
    "To prevent this, make sure to always mix in examples of what the model previously got correct. If you're training a new category \"website\", also include examples of \"person\". spaCy can help you with this. You can create those additional examples by running the existing model over data and extracting the entity spans you care about. You can then mix those examples in with your existing data and update the model with annotations of all labels.\n",
    "\n",
    "- For example, if you're training `WEBSITE`, also include examples of `PERSON`.\n",
    "- Run existing spaCy model over data and extract all other relevant entities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BAD\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})\n",
    "]\n",
    "\n",
    "# GOOD:\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    ('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "    ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: models can't learn anything\n",
    "\n",
    "Another common problem is that your model just won't learn what you want it to. spaCy's models make predictions based on the local context – for example, for named entities, the surrounding words are most important. If the decision is difficult to make based on the context, the model can struggle to learn it. The label scheme also needs to be consistent and not too specific. For example, it may be very difficult to teach a model to predict whether something is adult clothing or children's clothing based on the context. However, just predicting the label \"clothing\" may work better.\n",
    "\n",
    "- spaCy's models make predictions based on local context\n",
    "- Model can struggle to learn if decision is difficult to make based on context\n",
    "- Label scheme needs to be consistent and not too specific\n",
    "    - For example: `CLOTHING` is better than `ADULT_CLOTHING` and `CHILDRENS_CLOTHING`\n",
    "    \n",
    "### Solution 2: plan your label scheme carefully\n",
    "\n",
    "Before you start training and updating models, it's worth taking a step back and planning your label scheme. Try to pick categories that are reflected in the local context and make them more generic if possible. You can always add a rule-based system later to go from generic to specific. Generic categories like \"clothing\" or \"band\" are both easier to label and easier to learn.\n",
    "\n",
    "- Pick categories that are reflected in local context.\n",
    "- More generic is better than too specific.\n",
    "- Use rules to go from generic labels to specific categories."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BAD:\n",
    "LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE']\n",
    "\n",
    "#GOOD:\n",
    "LABELS = ['CLOTHING', 'BAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good data vs. bad data\n",
    "\n",
    "In the example below we have a number of entries, one of which is misspelled and where we have linguistic ambiguities. Even very uncommon words or misspellings can be labelled as entities. In fact, being able to predict categories in misspelled text based on the context is one of the big advantages of statistical named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"i went to amsterdem last year and the canals were beautiful\",\n",
    "        {\"entities\": [(10, 19, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\",\n",
    "        {\"entities\": [(17, 22, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {\"entities\": []}),\n",
    "    (\n",
    "        \"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\",\n",
    "        {\"entities\": [(0, 6, \"TOURIST_DESTINATION\")]},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it’s possible that Paris, AK is also a tourist attraction, this only highlights how subjective the label scheme is and how difficult it will be to decide whether the label applies or not. As a result, this distinction will also be very difficult to learn for the entity recognizer.\n",
    "\n",
    "A much better approach would be to only label `GPE` (geopolitical entity) or `LOCATION` and then use a rule-based system to determine whether the entity is a tourist destination in this context. For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training multiple labels\n",
    "\n",
    "Here’s a small sample of a dataset created to train a new entity type WEBSITE. The original dataset contains a few thousand sentences. In this exercise, you’ll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, [Brat](http://brat.nlplab.org/), a popular open-source solution, or [Prodigy](https://prodi.gy/), our own annotation tool that integrates with spaCy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The start and end of the website names were manually entered.\n",
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(18, 25, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model was trained with the data you just labelled, plus a few thousand similar examples. After training, it’s doing great on `WEBSITE`, but doesn’t recognize PERSON anymore. Why could this be happening?\n",
    "\n",
    "The reason is that if If `PERSON` entities occur in the training data but aren’t labelled, the model will learn that they shouldn’t be predicted. Similarly, if an existing entity type isn’t present in the training data, the model may ”forget” and stop predicting it. We need to include the `PERSON` entities appearing in the examples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(0, 9, \"PERSON\"), (18, 25, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (15, 29, \"PERSON\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Congratulations – you've made it to the end of the course!\n",
    "\n",
    "### Your new spaCy skills\n",
    "\n",
    "Here's an overview of all the new skills you learned so far:\n",
    "\n",
    "- In the first chapter, you learned how to extract linguistic features like part-of-speech tags, syntactic dependencies and named entities, and how to work with pre-trained statistical models.\n",
    "- You also learned to write powerful match patterns to extract words and phrases using spaCy's matcher and phrase matcher.\n",
    "- Chapter 2 was all about information extraction, and you learned how to work with the data structures, the Doc, Token and Span, as well as the vocab and lexical entries.\n",
    "- You also used spaCy to predict semantic similarities using word vectors.\n",
    "- In chapter 3, you got some more insights into spaCy's pipeline, and learned to write your own custom pipeline components that modify the Doc.\n",
    "- You also created your own custom extension attributes for Docs, Tokens and Spans, and learned about processing streams and making your pipeline faster.\n",
    "- Finally, in chapter 4, you learned about training and updating spaCy's statistical models, specifically the entity recognizer.\n",
    "- You learned some useful tricks for how to create training data, and how to design your label scheme to get the best results.\n",
    "\n",
    "In brief:\n",
    "\n",
    "- Extract linguistic features: part-of-speech tags, dependencies, named entities\n",
    "- Work with pre-trained statistical models\n",
    "- Find words and phrases using Matcher and PhraseMatcher match rules\n",
    "- Best practices for working with data structures Doc, Token Span, Vocab, Lexeme\n",
    "- Find semantic similarities using word vectors\n",
    "- Write custom pipeline components with extension attributes\n",
    "- Scale up your spaCy pipelines and make them fast\n",
    "- Create training data for spaCy' statistical models\n",
    "- Train and update spaCy's neural network models with new data\n",
    "\n",
    "### More things to do with spaCy (1)\n",
    "\n",
    "Of course, there's a lot more that spaCy can do that we didn't get to cover in this course. \n",
    "\n",
    "- While we focused mostly on training the entity recognizer, you can also train and update the other statistical pipeline components like the part-of-speech tagger and dependency parser. \n",
    "- Another useful pipeline component is the text classifier, which can learn to predict labels that apply to the whole text. \n",
    "- It's not part of the pre-trained models, but you can add it to an existing model and train it on your own data.\n",
    "\n",
    "In brief:\n",
    "\n",
    "- [Training and updating](https://spacy.io/usage/training) other pipeline components\n",
    "- Part-of-speech tagger\n",
    "- Dependency parser\n",
    "- Text classifier\n",
    "\n",
    "### More things you can do with spaCy (2)\n",
    "\n",
    "In this course, we basically accepted the default tokenization as it is. But you don't have to!\n",
    "\n",
    "- spaCy lets you customize the rules used to determine where and how to split the text.\n",
    "- You can also add and improve the support for other languages.\n",
    "- While spaCy already supports tokenization for many different languages, there's still a lot of room for improvement.\n",
    "- Supporting tokenization for a new language is the first step towards being able to train a statistical model.\n",
    "\n",
    "In brief:\n",
    "\n",
    "- [Customizing the tokenizer](https://spacy.io/usage/linguistic-features#tokenization)\n",
    "    - Adding rules and exceptions to split text differently\n",
    "- [Adding or improving support for other languages](https://spacy.io/usage/adding-languages)\n",
    "    - 45+ languages currently\n",
    "    - Lots of room for improvement and more languages\n",
    "    - Allows training models for other languages\n",
    "    \n",
    "### Search the website for more documentation\n",
    "\n",
    "For more examples, tutorials and in-depth API documentation, check out the [spaCy website](https://spacy.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
